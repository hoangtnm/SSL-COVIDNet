import streamlit as st
from pathlib import Path

IMG_DIR = Path("images")


def app():
    st.title("AI System for COVID-19 Diagnosis")
    st.write("""
        This application is a part of the [An Improvement in Medical Imaging via
        Self-Supervised Representation Learning]() paper. It has two main functionalities:
        (1) A dashboard for analyzing and visualizing the spread of COVID-19 around the world
        (2) A complementary tool for COVID-19 diagnosis powered by computer vision and deep learning.
    """)
    st.header("""
        Contents
        - Introduction
        - Functionalities
        - The Proposed Model: Self-Supervised COVIDNet (SSL-COVIDNet)
        - Experimental Results
    """)

    st.header("Introduction")
    st.write("""
        The project aims to develop an AI system for COVID-19 Diagnosis based on
        Computer Vision and Self-Supervised Learning. Compared to other approaches
        such as [COVNet](https://pubs.rsna.org/doi/10.1148/radiol.2020200905),
        [DeTraC](https://link.springer.com/article/10.1007/s10489-020-01829-7),
        [COVID-Net CT](https://arxiv.org/abs/2009.05383),
        [COVID-Net CT-2](https://arxiv.org/abs/2101.07433), this project proposes
        Self-Supervised COVIDNet (SSL-COVIDNet), which is which is
        a deep neural network designed for the Coronavirus disease (COVID-19)
        diagnosis from chest CT images based on self-supervised learning.
        The main ingredient of the model is Momentum Contrast (MoCo),
        which is a self-supervised learning algorithm with a contrastive loss.
        Therefore, it can capture more general image representations from unlabelled images.
    """)
    st.image(str(IMG_DIR / "dataset_examples.png"))
    st.write("""
        Figure 1. Example CT images from the COVIDx CT-2 dataset:
        (1) COVID-19 postitive, (2) common pneumonia and, (3) normal cases.
    """)

    st.header("""
        Functionalities
        - Dashboard for statistical analysis
        - Upload chest CT image for COVID-19 Diagnosis
    """)

    st.header("The Proposed Model: Self-Supervised COVIDNet")
    st.subheader("Self-Supervised Pretraining using MoCo")
    # st.markdown("""
    # """, unsafe_allow_html=True)
    st.markdown("""
        During the pretraining phase. A chest CT image is transformed via two
        random augmentations (Aug. 1 and Aug. 2) into images $x_q$ and $x_k$.
        The first transformed image is passed through an encoder network,
        while the second one is passed through a momentum encoder network.
        The representations generated by each encoder are then passed into a
        contrastive loss that computes the similarity
        between the representations $q$ and $k$.""")
    st.image(str(IMG_DIR / "moco_training.png"))
    st.write("""
        The following formula demonstrates the MoCo's formula modeled by
        InfoNCE loss function. During training, the dictionary is maintained as
        a queue: the encoded representations of the current batch are enqueued,
        and the oldest ones are dequeued. Then, the similarity between each
        encoded q and a set of K stored keys {$k_0 , k_1 , k_2 , ...$} is
        measured by *dot product*. Therefore, through the training procedure,
        similar ones become closer and vice versa.
    """)
    st.latex(r"""
       \mathcal{L}_q = -\log \frac{\exp(q{\cdot}k_+ / \tau)}{\sum_{i=0}^{K}\exp(q{\cdot}k_i  / \tau)}
    """)
    st.subheader("Supervised Fine-tuning")
    st.image(str(IMG_DIR / "sslcovidnet.png"))
    st.write("""
        The above figure illustrates the schematic of the SSL-COVIDNet.
        In general, it is a combination of the pretrained encoder and an
        FC layer for the task of classification.
    """)

    st.header("Experimental Results")
    st.subheader("Pretraining")
    st.image(str(IMG_DIR / "exp_pretrain_acc1.png"))
    st.write("""
        The above figure demonstrates that from the first 30.000 early steps of
        both training and validation, our proposed model is able to achieve a
        higher and more stable accuracy increase compared to the other models.
        Moreover, this trend tends to continue through the pretraining phase,
        and our proposed model is also able to achieve a higher accuracy of
        approximately 98% with the half time at the 45.000th training step.
    """)
    st.subheader("Fine-tuning")
    st.image(str(IMG_DIR / "finetune_acc1.png"))
    st.write("""
        The above figure demonstrates the accuracy during fine-tuning phase.
        For fine-tuning, weighted random sampling is applied to deal with the data imbalance.
        Interestingly, it can be seen that the model is able to reach the
        accuracy of approximately 84% on both training and validation set right
        from the first epoch, which is hard to achieve when using
        transfer learning via supervised learning.
        Moreover, this model tends to increase accuracy through time and reaches
        about 86.5% as its best accuracy on the validation set.
    """)
    st.latex(r"""
        prob^{c}_{sampling} = \frac{1}{\text{Total number of samples of class } c}
    """)

    st.header("Citation")
    st.info("""
        @inproceedings{sslcovidnet,\n
          author       = "Tran Nhat Minh Hoang, Tran The Son, Nguyen Duy Nghiem, and Le Minh Tuan",\n
          title        = "An Improvement in Medical Imaging via Self-Supervised Representation Learning",\n
          booktitle    = "",\n
          year         = "2021",\n
          pages        = "",\n
          publisher    = "",
          address   = "Danang, Vietnam"
        }
    """)
